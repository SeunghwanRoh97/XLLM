250319, 250320 ch5 두 파일은 연속적으로 실행하면 각각 RNN에 대한 IMDB 영화 긍정 부정 분류 결과와 BERT 결과가 나옵니다

250319 -> IMDB에 대한 토크나이제이션 결과가 생성 및 저장, 이를 RNN으로만 학습 및 평가
250320 -> 250319에서 만들어진 토큰 파일을 로드하여 프리프로세싱(truncation, padding, attention mask 생성) 및 BERT 로 학습 및 평가

개발환경은 torch 2.2.0 쿠다 11.8에 torchtext 0.17.0 입니다.

그래픽카드는 rtx 1080ti 이고 각각 데이터셋 준비 등을 제외하고 트레이닝과 평가에 30분 2시간 정도가 걸립니다.

결과는 의외로 RNN 쪽이 트랜스포머 구조와 크게 차이가 나지 않는 둘 다 0.94 정도로 유사하게 나왔는데 이는 BERT-BASE 모델이 MAX_TOKENS 가 512로 제한되어 모든 문맥을 받아들이지 못하고
truncation 해야 했기 때문으로 생각합니다.
사실 각 영화평의 token 수의 평균 값은 300 중반대이나 95% 99%에 해당하는 토큰수는 대략 1024 정도입니다. 때문에 rnn과는 달리 bert 모델은 모든 문맥을 받아들이지 못하고 학습한 영화평들이 꽤 있다는 얘기입니다.

추가적인 궁금증이 있다면 또 물어봐주시면 감사하겠습니다.